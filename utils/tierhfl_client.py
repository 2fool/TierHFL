import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import copy
import numpy as np
import math

# æ··åˆæŸå¤± - å¹³è¡¡ä¸ªæ€§åŒ–å’Œå…¨å±€æ€§èƒ½
class HybridLoss(nn.Module):
    def __init__(self, alpha=0.5):
        super(HybridLoss, self).__init__()
        self.alpha = alpha  # å¹³è¡¡å› å­
        self.criterion = nn.CrossEntropyLoss()
        
    def forward(self, local_logits, global_logits, target):
        local_loss = self.criterion(local_logits, target)
        global_loss = self.criterion(global_logits, target)
        return self.alpha * local_loss + (1 - self.alpha) * global_loss, local_loss, global_loss
    
    def update_alpha(self, alpha):
        """æ›´æ–°å¹³è¡¡å› å­"""
        self.alpha = alpha

# ç‰¹å¾å¯¹é½æŸå¤± - å¤„ç†ä¸åŒç»´åº¦çš„ç‰¹å¾
class EnhancedFeatureAlignmentLoss(nn.Module):
    def __init__(self):
        super(EnhancedFeatureAlignmentLoss, self).__init__()
        
    def forward(self, client_features, server_features, round_idx=0):
        """æ”¹è¿›çš„ç‰¹å¾å¯¹é½æŸå¤±è®¡ç®—"""
        # æ·»åŠ è°ƒè¯•æ¨¡å¼
        debug_mode = hasattr(self, '_debug_client_id') and self._debug_client_id == 6
        
        if debug_mode:
            print(f"\n[Feature Loss DEBUG] å®¢æˆ·ç«¯ç‰¹å¾å½¢çŠ¶: {client_features.shape}")
            print(f"[Feature Loss DEBUG] æœåŠ¡å™¨ç‰¹å¾å½¢çŠ¶: {server_features.shape}")
        
        # ç¡®ä¿ç‰¹å¾æ˜¯4Dæˆ–2Då¼ é‡
        if len(client_features.shape) == 4:  # 4D: [B, C, H, W]
            batch_size = client_features.size(0)
            client_pooled = F.adaptive_avg_pool2d(client_features, (1, 1))
            client_features = client_pooled.view(batch_size, -1)
            
            if debug_mode:
                print(f"[Feature Loss DEBUG] æ± åŒ–åå®¢æˆ·ç«¯ç‰¹å¾å½¢çŠ¶: {client_features.shape}")
        
        if len(server_features.shape) == 4:  # 4D: [B, C, H, W]
            batch_size = server_features.size(0)
            server_pooled = F.adaptive_avg_pool2d(server_features, (1, 1))
            server_features = server_pooled.view(batch_size, -1)
            
            if debug_mode:
                print(f"[Feature Loss DEBUG] æ± åŒ–åæœåŠ¡å™¨ç‰¹å¾å½¢çŠ¶: {server_features.shape}")
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§ - å¦‚æœä¸ä¸€è‡´è¯´æ˜æ¨¡å‹æ¶æ„æœ‰é—®é¢˜
        if client_features.size(1) != server_features.size(1):
            if debug_mode:
                print(f"[Feature Loss DEBUG] ç‰¹å¾ç»´åº¦ä¸åŒ¹é…! å®¢æˆ·ç«¯: {client_features.size(1)}, æœåŠ¡å™¨: {server_features.size(1)}")
            
            # ğŸ”¥ ä¸å†è£åˆ‡ï¼Œè€Œæ˜¯æŠ¥è­¦å‘Šå¹¶è¿”å›é›¶æŸå¤±ï¼Œé¿å…ä¿¡æ¯æŸå¤±
            logging.warning(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼šå®¢æˆ·ç«¯{client_features.size(1)} vs æœåŠ¡å™¨{server_features.size(1)}ï¼Œè·³è¿‡ç‰¹å¾å¯¹é½æŸå¤±è®¡ç®—")
            return torch.tensor(0.0, device=client_features.device, requires_grad=True)
        
        # æ ‡å‡†åŒ–ç‰¹å¾å‘é‡å¹¶æ£€æµ‹å¼‚å¸¸å€¼
        try:
            client_norm = F.normalize(client_features, dim=1)
            server_norm = F.normalize(server_features, dim=1)
            
            if debug_mode:
                print(f"[Feature Loss DEBUG] å®¢æˆ·ç«¯å½’ä¸€åŒ–åæ˜¯å¦æœ‰NaN: {torch.isnan(client_norm).any().item()}")
                print(f"[Feature Loss DEBUG] æœåŠ¡å™¨å½’ä¸€åŒ–åæ˜¯å¦æœ‰NaN: {torch.isnan(server_norm).any().item()}")
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            cosine_sim = torch.mean(torch.sum(client_norm * server_norm, dim=1))
            cosine_loss = 1.0 - cosine_sim
            
            if debug_mode:
                print(f"[Feature Loss DEBUG] ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim.item():.4f}")
                print(f"[Feature Loss DEBUG] ç‰¹å¾å¯¹é½æŸå¤±: {cosine_loss.item():.4f}")
        except Exception as e:
            if debug_mode:
                print(f"[Feature Loss DEBUG] è®¡ç®—ç‰¹å¾å¯¹é½æŸå¤±å‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›ä¸€ä¸ªé»˜è®¤æŸå¤±å€¼
            return torch.tensor(1.0, device=client_features.device)
        
        # éšè®­ç»ƒè½®æ¬¡æ¸è¿›å¢å¼ºç‰¹å¾å¯¹é½å¼ºåº¦
        alignment_weight = min(0.8, 0.2 + round_idx/100)
        
        return cosine_loss * alignment_weight

class TierHFLClient:
    """TierHFLå®¢æˆ·ç«¯ç±»"""
    def __init__(self, client_id, tier, train_data, test_data, device='cuda', 
                 lr=0.001, local_epochs=1):
        self.client_id = client_id
        self.tier = tier
        self.train_data = train_data
        self.test_data = test_data
        # [MARK-CLIENT-DEVICE] normalize device
        if not isinstance(device, torch.device):
            device = torch.device(device)
        self.device = device
        self.lr = lr
        self.local_epochs = local_epochs
        
        # æ¨¡å‹å¼•ç”¨ - å°†åœ¨è®­ç»ƒæ—¶è®¾ç½®
        self.model = None
        
        # è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'train_loss': [],
            'train_acc': [],
            'local_loss': [],
            'global_loss': [],
            'feature_loss': []
        }
        
        # åŠ¨æ€å‚æ•°
        self.alpha = 0.5  # æœ¬åœ°æŸå¤±æƒé‡
        self.lambda_feature = 0.1  # ç‰¹å¾å¯¹é½æŸå¤±æƒé‡
        
        # AMP æ”¯æŒ
        self.use_amp = (isinstance(device, torch.device) and device.type == "cuda")
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp) if self.use_amp else None
    
    def update_learning_rate(self, lr_factor=0.85):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self.lr *= lr_factor
        return self.lr
    
    def train_personalized_layers(self, round_idx=0, total_rounds=100):
        """åªè®­ç»ƒä¸ªæ€§åŒ–å±‚ï¼Œå¹¶æ”¶é›†å…±äº«å±‚ç‰¹å¾"""
        if self.model is None:
            raise ValueError("å®¢æˆ·ç«¯æ¨¡å‹æœªè®¾ç½®")
        
        self.model.train()
        
        # å†»ç»“å…±äº«å±‚
        for name, param in self.model.named_parameters():
            if 'shared_base' in name:
                param.requires_grad = False
            else:
                param.requires_grad = True
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - åªä¼˜åŒ–ä¸ªæ€§åŒ–å±‚
        optimizer = torch.optim.Adam(
            [p for n, p in self.model.named_parameters() if 'shared_base' not in n],
            lr=self.lr
        )
        
        # å¯é€‰ï¼šåˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.7, patience=3, verbose=False
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'local_loss': 0.0,
            'correct': 0,
            'total': 0,
            'batch_count': 0
        }
        
        # ä¿å­˜çš„ç‰¹å¾æ•°æ®
        features_data = []
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è®¡ç®—è¿›åº¦å› å­ - ç”¨äºåŠ¨æ€è°ƒæ•´è¶…å‚æ•°
        progress = round_idx / max(1, total_rounds)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.local_epochs):
            epoch_loss = 0.0
            epoch_correct = 0
            epoch_total = 0
            
            for batch_idx, (data, target) in enumerate(self.train_data):
                # ç§»è‡³è®¾å¤‡ - æ·»åŠ  non_blocking=True
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                # æ¸…é™¤æ¢¯åº¦
                optimizer.zero_grad(set_to_none=True)
                
                # å‰å‘ä¼ æ’­ - æ·»åŠ  AMP æ”¯æŒ
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    local_logits, shared_features, personal_features = self.model(data)
                    local_loss = F.cross_entropy(local_logits, target)
                
                # åå‘ä¼ æ’­ - æ·»åŠ  AMP æ”¯æŒ
                if self.use_amp and self.scaler is not None:
                    self.scaler.scale(local_loss).backward()
                    self.scaler.step(optimizer)
                    self.scaler.update()
                else:
                    local_loss.backward()
                    optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                epoch_loss += local_loss.item()
                stats['local_loss'] += local_loss.item()
                stats['batch_count'] += 1
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, pred = local_logits.max(1)
                batch_correct = pred.eq(target).sum().item()
                batch_total = target.size(0)
                
                epoch_correct += batch_correct
                epoch_total += batch_total
                stats['correct'] += batch_correct
                stats['total'] += batch_total
                
                # ä¿å­˜ç‰¹å¾æ•°æ®
                features_data.append({
                    'shared_features': shared_features.detach(),
                    'personal_features': personal_features.detach(),
                    'targets': target.clone(),
                    'local_loss': local_loss.item()
                })
            
            # æ¯è½®ç»“æŸåæ›´æ–°å­¦ä¹ ç‡
            epoch_acc = 100.0 * epoch_correct / max(1, epoch_total)
            scheduler.step(epoch_acc)
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ€»ä½“å‡†ç¡®ç‡
        avg_local_loss = stats['local_loss'] / max(1, stats['batch_count'])
        avg_local_acc = 100.0 * stats['correct'] / max(1, stats['total'])
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats['local_loss'].append(avg_local_loss)
        self.stats['train_acc'].append(avg_local_acc)
        
        return {
            'local_loss': avg_local_loss,
            'local_accuracy': avg_local_acc,
            'time_cost': time.time() - start_time,
            'features_data': features_data
        }
    
    def evaluate(self, server_model, global_classifier):
        """è¯„ä¼°å®¢æˆ·ç«¯æ¨¡å‹"""
        if self.model is None:
            raise ValueError("å®¢æˆ·ç«¯æ¨¡å‹æœªè®¾ç½®")

        # å…³é”®ï¼šæŠŠæœåŠ¡ç«¯ä¸å…¨å±€å¤´æ¬åˆ°åŒä¸€ä¸ª device
        server_model       = server_model.to(self.device)
        global_classifier  = global_classifier.to(self.device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        server_model.eval()
        global_classifier.eval()
        
        # ç»Ÿè®¡ä¿¡æ¯
        local_correct = 0
        global_correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.test_data:
                # ç§»åˆ°è®¾å¤‡ - æ·»åŠ  non_blocking=True
                # [MARK-NONBLOCK-EVAL]
                data   = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                # å‰å‘ä¼ æ’­ - æ·»åŠ  AMP æ”¯æŒ
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    local_logits, shared_features, _ = self.model(data)
                    server_features = server_model(shared_features)
                    global_logits = global_classifier(server_features)
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, local_pred = local_logits.max(1)
                _, global_pred = global_logits.max(1)
                
                local_correct += local_pred.eq(target).sum().item()
                global_correct += global_pred.eq(target).sum().item()
                total += target.size(0)
        
        # è®¡ç®—å‡†ç¡®ç‡
        local_accuracy = 100.0 * local_correct / max(1, total)
        global_accuracy = 100.0 * global_correct / max(1, total)
        
        return {
            'local_accuracy': local_accuracy,
            'global_accuracy': global_accuracy,
            'total_samples': total
        }
    
    def apply_shared_layer_gradients(self, shared_grads):
        """åº”ç”¨æœåŠ¡å™¨è®¡ç®—çš„å…±äº«å±‚æ¢¯åº¦"""
        if self.model is None:
            raise ValueError("å®¢æˆ·ç«¯æ¨¡å‹æœªè®¾ç½®")
            
        # åˆ›å»ºä¼˜åŒ–å™¨
        shared_optimizer = torch.optim.Adam(
            [p for n, p in self.model.named_parameters() if 'shared_base' in n],
            lr=self.lr * 0.5  # å…±äº«å±‚ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
        )
        
        # åº”ç”¨æ¢¯åº¦
        for name, param in self.model.named_parameters():
            if 'shared_base' in name and name in shared_grads:
                if param.grad is None:
                    param.grad = shared_grads[name].clone()
                else:
                    param.grad.copy_(shared_grads[name])
        
        # æ›´æ–°å‚æ•°
        shared_optimizer.step()
        shared_optimizer.zero_grad(set_to_none=True)
        
        return True
    
    def update_alpha(self, alpha):
        """æ›´æ–°æœ¬åœ°å’Œå…¨å±€æŸå¤±çš„å¹³è¡¡å› å­"""
        self.alpha = alpha
    
    def update_lambda_feature(self, lambda_feature):
        """æ›´æ–°ç‰¹å¾å¯¹é½æŸå¤±æƒé‡"""
        self.lambda_feature = lambda_feature

# å®¢æˆ·ç«¯ç®¡ç†å™¨
class TierHFLClientManager:
    def __init__(self):
        self.clients = {}
        self.default_device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
    # åœ¨å®¢æˆ·ç«¯ç®¡ç†å™¨ä¸­æ·»åŠ ç›‘æ§æ–¹æ³•
    def add_client(self, client_id, tier, train_data, test_data, device=None, lr=0.001, local_epochs=1):
        """æ·»åŠ å®¢æˆ·ç«¯"""
        device = device or self.default_device
        
        client = TierHFLClient(
            client_id=client_id,
            tier=tier,
            train_data=train_data,
            test_data=test_data,
            device=device,
            lr=lr,
            local_epochs=local_epochs
        )
        
        # é’ˆå¯¹å®¢æˆ·ç«¯6æ·»åŠ ç‰¹æ®Šç›‘æ§
        if client_id == 6:
            print(f"\n[CLIENT MANAGER] æ³¨å†Œå®¢æˆ·ç«¯6 - Tier: {tier}")
            print(f"[CLIENT MANAGER] å®¢æˆ·ç«¯6è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data.dataset)}")
            print(f"[CLIENT MANAGER] å®¢æˆ·ç«¯6æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data.dataset)}")
            
            # æ£€æŸ¥æ•°æ®é›†åˆ†å¸ƒ
            try:
                # è·å–å‰5ä¸ªæ ·æœ¬çš„æ ‡ç­¾
                print("[CLIENT MANAGER] åˆ†æå®¢æˆ·ç«¯6æ•°æ®é›†...")
                sample_labels = []
                for i, (_, labels) in enumerate(train_data):
                    sample_labels.extend(labels.tolist())
                    if i >= 2:  # åªæ£€æŸ¥å‰å‡ ä¸ªæ‰¹æ¬¡
                        break
                
                # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
                label_counts = {}
                for label in sample_labels:
                    label_counts[label] = label_counts.get(label, 0) + 1
                    
                print(f"[CLIENT MANAGER] å®¢æˆ·ç«¯6è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ(éƒ¨åˆ†): {label_counts}")
            except Exception as e:
                print(f"[CLIENT MANAGER] åˆ†æå®¢æˆ·ç«¯6æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        
        self.clients[client_id] = client
        return client
    
    def get_client(self, client_id):
        """è·å–å®¢æˆ·ç«¯"""
        return self.clients.get(client_id)
    
    def update_client_tier(self, client_id, new_tier):
        """æ›´æ–°å®¢æˆ·ç«¯çš„tierçº§åˆ«"""
        if client_id in self.clients:
            self.clients[client_id].tier = new_tier
            return True
        return False
    
    def update_client_alpha(self, client_id, alpha):
        """æ›´æ–°å®¢æˆ·ç«¯çš„alphaå€¼"""
        if client_id in self.clients:
            self.clients[client_id].update_alpha(alpha)
            return True
        return False
    
    def update_client_feature_lambda(self, client_id, lambda_feature):
        """æ›´æ–°å®¢æˆ·ç«¯çš„ç‰¹å¾å¯¹é½æŸå¤±æƒé‡"""
        if client_id in self.clients:
            self.clients[client_id].update_lambda_feature(lambda_feature)
            return True
        return False
    
    def update_all_clients_alpha(self, alpha):
        """æ›´æ–°æ‰€æœ‰å®¢æˆ·ç«¯çš„alphaå€¼"""
        for client in self.clients.values():
            client.update_alpha(alpha)
    
    def update_all_clients_feature_lambda(self, lambda_feature):
        """æ›´æ–°æ‰€æœ‰å®¢æˆ·ç«¯çš„ç‰¹å¾å¯¹é½æŸå¤±æƒé‡"""
        for client in self.clients.values():
            client.update_lambda_feature(lambda_feature)
