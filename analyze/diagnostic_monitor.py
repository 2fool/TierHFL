import torch
import torch.nn.functional as F
import numpy as np
import logging
import copy
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import os
import warnings
warnings.filterwarnings("ignore")

class EnhancedTierHFLDiagnosticMonitor:
    """å¢å¼ºç‰ˆTierHFLè¯Šæ–­ç›‘æ§å™¨ - ä¿®å¤é”™è¯¯å¹¶æ·»åŠ æ ¸å¿ƒé—®é¢˜æ’æŸ¥"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.gradient_history = []
        self.feature_history = []
        self.loss_history = []
        self.model_stability_history = []
        
        # ç›‘æ§æŒ‡æ ‡
        self.metrics = {
            'gradient_conflict': [],
            'gradient_norm_ratio': [],
            'feature_similarity': [],
            'feature_diversity': [],
            'feature_stability': [],
            'loss_components': [],
            'model_parameter_change': [],
            'classification_confidence': [],
            'classifier_collapse': [],
            'weight_distribution': [],
            'shared_layer_quality': [],
            'aggregation_analysis': []
        }
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger("EnhancedDiagnosticMonitor")
        
        # æ ¸å¿ƒé—®é¢˜è¿½è¸ª
        self.collapse_detected = False
        self.previous_predictions = None
        
    def analyze_gradient_conflict_fixed(self, client_model, global_loss, local_loss, round_idx, client_id):
        """ä¿®å¤ç‰ˆæ¢¯åº¦å†²çªåˆ†æ"""
        try:
            # æ£€æŸ¥æŸå¤±æ˜¯å¦éœ€è¦æ¢¯åº¦
            if not isinstance(global_loss, torch.Tensor) or not isinstance(local_loss, torch.Tensor):
                self.logger.debug(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: æŸå¤±ä¸æ˜¯å¼ é‡ç±»å‹")
                return {}
            
            # ç¡®ä¿æŸå¤±éœ€è¦æ¢¯åº¦
            if not global_loss.requires_grad:
                self.logger.debug(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: å…¨å±€æŸå¤±ä¸éœ€è¦æ¢¯åº¦")
                return {}
                
            # è·å–å…±äº«å±‚å‚æ•°
            shared_params = []
            for name, param in client_model.named_parameters():
                if 'shared_base' in name and param.requires_grad and param.grad is None:
                    shared_params.append(param)
            
            if not shared_params:
                self.logger.debug(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å…±äº«å±‚å‚æ•°")
                return {}
            
            # è®¡ç®—å…¨å±€æŸå¤±æ¢¯åº¦
            try:
                global_grads = torch.autograd.grad(
                    global_loss, shared_params, retain_graph=True, 
                    create_graph=False, allow_unused=True
                )
            except Exception as e:
                self.logger.debug(f"å…¨å±€æŸå¤±æ¢¯åº¦è®¡ç®—å¤±è´¥: {str(e)}")
                return {}
            
            # è®¡ç®—æœ¬åœ°æŸå¤±æ¢¯åº¦ - ä¿®å¤ï¼šå¤„ç†frozenå‚æ•°
            if hasattr(local_loss, 'requires_grad') and local_loss.requires_grad:
                try:
                    local_grads = torch.autograd.grad(
                        local_loss, shared_params, retain_graph=True, 
                        create_graph=False, allow_unused=True
                    )
                except Exception as e:
                    self.logger.debug(f"æœ¬åœ°æŸå¤±æ¢¯åº¦è®¡ç®—å¤±è´¥: {str(e)}")
                    # å¦‚æœæœ¬åœ°æŸå¤±æ¢¯åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é›¶æ¢¯åº¦ä½œä¸ºæ›¿ä»£
                    local_grads = [torch.zeros_like(p) for p in shared_params]
            else:
                # æœ¬åœ°æŸå¤±ä¸éœ€è¦æ¢¯åº¦ï¼Œä½¿ç”¨é›¶æ¢¯åº¦
                local_grads = [torch.zeros_like(p) for p in shared_params]
            
            # è®¡ç®—æ¢¯åº¦ç›¸ä¼¼åº¦
            similarities = []
            norm_ratios = []
            
            for g_grad, l_grad in zip(global_grads, local_grads):
                if g_grad is not None and l_grad is not None:
                    g_flat = g_grad.flatten()
                    l_flat = l_grad.flatten()
                    
                    g_norm = torch.norm(g_flat)
                    l_norm = torch.norm(l_flat)
                    
                    if g_norm > 1e-8 and l_norm > 1e-8:
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        cos_sim = F.cosine_similarity(g_flat.unsqueeze(0), l_flat.unsqueeze(0))
                        similarities.append(cos_sim.item())
                        
                        # è®¡ç®—èŒƒæ•°æ¯”ç‡
                        norm_ratios.append((g_norm / l_norm).item())
            
            if not similarities:
                return {}
            
            avg_similarity = np.mean(similarities)
            avg_norm_ratio = np.mean(norm_ratios)
            
            result = {
                'round': round_idx,
                'client_id': client_id,
                'avg_cosine_similarity': avg_similarity,
                'avg_norm_ratio': avg_norm_ratio,
                'conflict_level': self._classify_conflict_level(avg_similarity),
                'valid_grad_pairs': len(similarities)
            }
            
            self.metrics['gradient_conflict'].append(result)
            
            if avg_similarity < -0.1:
                self.logger.warning(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: ä¸¥é‡æ¢¯åº¦å†²çª! ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ¢¯åº¦å†²çªåˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def monitor_model_stability_fixed(self, model_state_dict, round_idx, model_type="client"):
        """ä¿®å¤ç‰ˆæ¨¡å‹ç¨³å®šæ€§ç›‘æ§"""
        try:
            if not hasattr(self, f'previous_{model_type}_state'):
                setattr(self, f'previous_{model_type}_state', copy.deepcopy(model_state_dict))
                return {}
            
            previous_state = getattr(self, f'previous_{model_type}_state')
            
            param_changes = []
            param_norms = []
            
            for name, param in model_state_dict.items():
                if name in previous_state:
                    # ç¡®ä¿å‚æ•°æ˜¯æµ®ç‚¹å‹å¼ é‡
                    if not param.dtype.is_floating_point:
                        continue
                    
                    prev_param = previous_state[name]
                    if not prev_param.dtype.is_floating_point:
                        continue
                    
                    # è®¡ç®—å‚æ•°å˜åŒ–
                    try:
                        change = torch.norm(param.float() - prev_param.float()).item()
                        param_changes.append(change)
                        
                        # å‚æ•°èŒƒæ•°
                        norm = torch.norm(param.float()).item()
                        param_norms.append(norm)
                    except Exception as e:
                        self.logger.debug(f"å‚æ•°{name}ç¨³å®šæ€§è®¡ç®—å¤±è´¥: {str(e)}")
                        continue
            
            if not param_changes:
                return {}
            
            avg_change = np.mean(param_changes)
            max_change = np.max(param_changes)
            avg_norm = np.mean(param_norms)
            
            stability_analysis = {
                'round': round_idx,
                'model_type': model_type,
                'avg_parameter_change': avg_change,
                'max_parameter_change': max_change, 
                'avg_parameter_norm': avg_norm,
                'stability_score': self._compute_stability_score(avg_change, avg_norm),
                'param_count': len(param_changes)
            }
            
            self.metrics['model_parameter_change'].append(stability_analysis)
            
            # æ›´æ–°å†å²çŠ¶æ€
            setattr(self, f'previous_{model_type}_state', copy.deepcopy(model_state_dict))
            
            if stability_analysis['stability_score'] < 0.2:
                self.logger.warning(f"è½®æ¬¡{round_idx} {model_type}æ¨¡å‹: å‚æ•°å˜åŒ–è¿‡å¤§, ç¨³å®šæ€§å¾—åˆ†: {stability_analysis['stability_score']:.4f}")
            
            return stability_analysis
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ç¨³å®šæ€§ç›‘æ§å¤±è´¥: {str(e)}")
            return {}
    
    def detect_classifier_collapse(self, logits, targets, round_idx, client_id, path_type="global"):
        """æ£€æµ‹åˆ†ç±»å™¨å´©æºƒ"""
        try:
            with torch.no_grad():
                # è®¡ç®—é¢„æµ‹åˆ†å¸ƒ - åŠ¨æ€è·å–ç±»åˆ«æ•°
                _, predictions = torch.max(logits, dim=1)
                # ä»logitsç»´åº¦æˆ–targetsæœ€å¤§å€¼è·å–ç±»åˆ«æ•°
                num_classes = max(logits.shape[1] if len(logits.shape) > 1 else 10, 
                                int(targets.max().item()) + 1 if len(targets) > 0 else 10)
                pred_counts = torch.bincount(predictions, minlength=num_classes)
                pred_distribution = pred_counts.float() / pred_counts.sum()
                
                # è®¡ç®—ç†µï¼ˆå¤šæ ·æ€§æŒ‡æ ‡ï¼‰
                entropy = -torch.sum(pred_distribution * torch.log(pred_distribution + 1e-8)).item()
                max_entropy = np.log(num_classes)  # ä½¿ç”¨åŠ¨æ€ç±»åˆ«æ•°è®¡ç®—æœ€å¤§ç†µ
                normalized_entropy = entropy / max_entropy
                
                # æ£€æµ‹å´©æºƒæ¨¡å¼
                max_class_ratio = pred_distribution.max().item()
                num_active_classes = (pred_distribution > 0.01).sum().item()  # æ´»è·ƒç±»åˆ«æ•°
                
                # å´©æºƒåˆ¤æ–­æ ‡å‡†
                is_collapsed = (max_class_ratio > 0.8) or (num_active_classes < 3) or (normalized_entropy < 0.3)
                
                collapse_analysis = {
                    'round': round_idx,
                    'client_id': client_id,
                    'path_type': path_type,
                    'max_class_ratio': max_class_ratio,
                    'num_active_classes': num_active_classes,
                    'normalized_entropy': normalized_entropy,
                    'is_collapsed': is_collapsed,
                    'pred_distribution': pred_distribution.cpu().numpy().tolist()
                }
                
                self.metrics['classifier_collapse'].append(collapse_analysis)
                
                if is_collapsed:
                    self.collapse_detected = True
                    self.logger.error(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id} {path_type}è·¯å¾„: æ£€æµ‹åˆ°åˆ†ç±»å™¨å´©æºƒ!")
                    self.logger.error(f"  - æœ€å¤§ç±»åˆ«å æ¯”: {max_class_ratio:.4f}")
                    self.logger.error(f"  - æ´»è·ƒç±»åˆ«æ•°: {num_active_classes}")
                    self.logger.error(f"  - æ ‡å‡†åŒ–ç†µ: {normalized_entropy:.4f}")
                
                return collapse_analysis
                
        except Exception as e:
            self.logger.error(f"åˆ†ç±»å™¨å´©æºƒæ£€æµ‹å¤±è´¥: {str(e)}")
            return {}
    
    def analyze_classifier_weights(self, classifier_model, round_idx):
        """åˆ†æåˆ†ç±»å™¨æƒé‡åˆ†å¸ƒ"""
        try:
            weight_analysis = {
                'round': round_idx,
                'layers': {}
            }
            
            for name, param in classifier_model.named_parameters():
                if 'weight' in name and param.dim() > 1:
                    weight_data = param.data.cpu().float()
                    
                    # åŸºæœ¬ç»Ÿè®¡
                    mean_val = weight_data.mean().item()
                    std_val = weight_data.std().item()
                    min_val = weight_data.min().item()
                    max_val = weight_data.max().item()
                    
                    # æƒé‡åˆ†å¸ƒåˆ†æ
                    weight_range = max_val - min_val
                    zero_ratio = (weight_data.abs() < 1e-6).float().mean().item()
                    
                    # æ£€æŸ¥æƒé‡å´©æºƒ
                    is_collapsed = (std_val < 1e-4) or (zero_ratio > 0.9) or (weight_range < 1e-4)
                    
                    layer_analysis = {
                        'mean': mean_val,
                        'std': std_val,
                        'min': min_val,
                        'max': max_val,
                        'range': weight_range,
                        'zero_ratio': zero_ratio,
                        'is_collapsed': is_collapsed
                    }
                    
                    weight_analysis['layers'][name] = layer_analysis
                    
                    if is_collapsed:
                        self.logger.warning(f"è½®æ¬¡{round_idx} åˆ†ç±»å™¨å±‚{name}: æƒé‡å´©æºƒ!")
                        self.logger.warning(f"  - æ ‡å‡†å·®: {std_val:.6f}")
                        self.logger.warning(f"  - é›¶å€¼æ¯”ä¾‹: {zero_ratio:.4f}")
            
            self.metrics['weight_distribution'].append(weight_analysis)
            return weight_analysis
            
        except Exception as e:
            self.logger.error(f"åˆ†ç±»å™¨æƒé‡åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def analyze_shared_layer_quality(self, shared_features, round_idx, client_id):
        """åˆ†æå…±äº«å±‚ç‰¹å¾è´¨é‡"""
        try:
            if shared_features is None or not isinstance(shared_features, torch.Tensor):
                return {}
            
            # ç¡®ä¿ç‰¹å¾æ˜¯2D
            if shared_features.dim() > 2:
                shared_features = F.adaptive_avg_pool2d(shared_features, (1, 1)).flatten(1)
            
            with torch.no_grad():
                # ç‰¹å¾æ¿€æ´»ç»Ÿè®¡
                mean_activation = shared_features.mean().item()
                std_activation = shared_features.std().item()
                max_activation = shared_features.max().item()
                min_activation = shared_features.min().item()
                
                # æ­»ç¥ç»å…ƒæ£€æµ‹
                dead_threshold = 1e-6
                dead_neurons = (shared_features.abs() < dead_threshold).all(dim=0).sum().item()
                total_neurons = shared_features.size(1)
                dead_ratio = dead_neurons / total_neurons if total_neurons > 0 else 0
                
                # ç‰¹å¾å¤šæ ·æ€§
                feature_diversity = self._compute_feature_diversity_robust(shared_features)
                
                # ç‰¹å¾é¥±å’Œåº¦ï¼ˆæ¿€æ´»å€¼æ¥è¿‘æå€¼çš„æ¯”ä¾‹ï¼‰
                saturation_ratio = ((shared_features.abs() > 0.9 * max_activation).sum().item() / 
                                  shared_features.numel() if shared_features.numel() > 0 else 0)
                
                quality_analysis = {
                    'round': round_idx,
                    'client_id': client_id,
                    'mean_activation': mean_activation,
                    'std_activation': std_activation,
                    'activation_range': max_activation - min_activation,
                    'dead_neuron_ratio': dead_ratio,
                    'feature_diversity': feature_diversity,
                    'saturation_ratio': saturation_ratio,
                    'quality_score': 1.0 - dead_ratio - saturation_ratio
                }
                
                self.metrics['shared_layer_quality'].append(quality_analysis)
                
                # æ£€æµ‹è´¨é‡é—®é¢˜
                if dead_ratio > 0.3:
                    self.logger.warning(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: å…±äº«å±‚æ­»ç¥ç»å…ƒè¿‡å¤š ({dead_ratio:.2%})")
                
                if feature_diversity < 0.2:
                    self.logger.warning(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: å…±äº«å±‚ç‰¹å¾å¤šæ ·æ€§ä¸è¶³ ({feature_diversity:.4f})")
                
                if saturation_ratio > 0.5:
                    self.logger.warning(f"è½®æ¬¡{round_idx} å®¢æˆ·ç«¯{client_id}: å…±äº«å±‚ç‰¹å¾é¥±å’Œåº¦è¿‡é«˜ ({saturation_ratio:.2%})")
                
                return quality_analysis
                
        except Exception as e:
            self.logger.error(f"å…±äº«å±‚è´¨é‡åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def analyze_aggregation_weights(self, aggregation_weights, round_idx):
        """åˆ†æèšåˆæƒé‡åˆ†å¸ƒ"""
        try:
            if not aggregation_weights:
                return {}
            
            weights_array = np.array(list(aggregation_weights.values()))
            
            analysis = {
                'round': round_idx,
                'weights': dict(aggregation_weights),
                'mean_weight': weights_array.mean(),
                'std_weight': weights_array.std(),
                'max_weight': weights_array.max(),
                'min_weight': weights_array.min(),
                'weight_imbalance': weights_array.max() / (weights_array.min() + 1e-8),
                'is_balanced': weights_array.std() < 0.2
            }
            
            self.metrics['aggregation_analysis'].append(analysis)
            
            if analysis['weight_imbalance'] > 10:
                self.logger.warning(f"è½®æ¬¡{round_idx}: èšåˆæƒé‡ä¸¥é‡ä¸å¹³è¡¡ (æ¯”ä¾‹: {analysis['weight_imbalance']:.2f})")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"èšåˆæƒé‡åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def monitor_learning_rates(self, client_lr_dict, round_idx):
        """ç›‘æ§å­¦ä¹ ç‡å˜åŒ–"""
        try:
            lr_analysis = {
                'round': round_idx,
                'learning_rates': dict(client_lr_dict),
                'avg_lr': np.mean(list(client_lr_dict.values())),
                'lr_std': np.std(list(client_lr_dict.values())),
                'max_lr': max(client_lr_dict.values()),
                'min_lr': min(client_lr_dict.values())
            }
            
            # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡é«˜æˆ–è¿‡ä½
            if lr_analysis['max_lr'] > 0.1:
                self.logger.warning(f"è½®æ¬¡{round_idx}: å­¦ä¹ ç‡è¿‡é«˜ (æœ€å¤§: {lr_analysis['max_lr']:.6f})")
            
            if lr_analysis['min_lr'] < 1e-6:
                self.logger.warning(f"è½®æ¬¡{round_idx}: å­¦ä¹ ç‡è¿‡ä½ (æœ€å°: {lr_analysis['min_lr']:.6f})")
            
            return lr_analysis
            
        except Exception as e:
            self.logger.error(f"å­¦ä¹ ç‡ç›‘æ§å¤±è´¥: {str(e)}")
            return {}
    
    def comprehensive_diagnostic_report(self, round_idx):
        """ç”Ÿæˆç»¼åˆè¯Šæ–­æŠ¥å‘Š"""
        try:
            report = {
                'round': round_idx,
                'critical_issues': [],
                'warnings': [],
                'recommendations': [],
                'overall_health': 'unknown'
            }
            
            # æ£€æŸ¥åˆ†ç±»å™¨å´©æºƒ
            recent_collapses = [m for m in self.metrics['classifier_collapse'] 
                              if m['round'] == round_idx and m['is_collapsed']]
            
            if recent_collapses:
                report['critical_issues'].append(f"æ£€æµ‹åˆ°{len(recent_collapses)}ä¸ªåˆ†ç±»å™¨å´©æºƒ")
                report['recommendations'].append("ç«‹å³æ£€æŸ¥åˆ†ç±»å™¨æ¶æ„å’Œå­¦ä¹ ç‡è®¾ç½®")
            
            # æ£€æŸ¥æƒé‡å´©æºƒ
            recent_weights = [m for m in self.metrics['weight_distribution'] if m['round'] == round_idx]
            collapsed_layers = 0
            for weight_analysis in recent_weights:
                for layer_name, layer_info in weight_analysis['layers'].items():
                    if layer_info['is_collapsed']:
                        collapsed_layers += 1
            
            if collapsed_layers > 0:
                report['critical_issues'].append(f"æ£€æµ‹åˆ°{collapsed_layers}ä¸ªæƒé‡å´©æºƒå±‚")
                report['recommendations'].append("é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æƒé‡åˆå§‹åŒ–æ–¹å·®")
            
            # æ£€æŸ¥ç‰¹å¾è´¨é‡
            recent_quality = [m for m in self.metrics['shared_layer_quality'] if m['round'] == round_idx]
            poor_quality_clients = [m for m in recent_quality if m['quality_score'] < 0.3]
            
            if poor_quality_clients:
                report['warnings'].append(f"{len(poor_quality_clients)}ä¸ªå®¢æˆ·ç«¯å…±äº«å±‚è´¨é‡è¾ƒå·®")
                report['recommendations'].append("è€ƒè™‘è°ƒæ•´å…±äº«å±‚æ¶æ„æˆ–å¢åŠ æ­£åˆ™åŒ–")
            
            # æ£€æŸ¥æ¢¯åº¦å†²çª
            recent_conflicts = [m for m in self.metrics['gradient_conflict'] 
                              if m['round'] == round_idx and m['avg_cosine_similarity'] < -0.1]
            
            if recent_conflicts:
                report['warnings'].append(f"æ£€æµ‹åˆ°{len(recent_conflicts)}ä¸ªä¸¥é‡æ¢¯åº¦å†²çª")
                report['recommendations'].append("è°ƒæ•´æŸå¤±æƒé‡å¹³è¡¡æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª")
            
            # æ•´ä½“å¥åº·è¯„ä¼°
            if report['critical_issues']:
                report['overall_health'] = 'critical'
            elif len(report['warnings']) > 2:
                report['overall_health'] = 'poor'
            elif report['warnings']:
                report['overall_health'] = 'warning'
            else:
                report['overall_health'] = 'good'
            
            return report
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆè¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return {'round': round_idx, 'overall_health': 'error'}
    
    def export_metrics_to_wandb(self, wandb_logger):
        """å¯¼å‡ºæ ¸å¿ƒæŒ‡æ ‡åˆ°wandb"""
        try:
            if not self.metrics['classifier_collapse']:
                return
            
            latest_round = max([m['round'] for m in self.metrics['classifier_collapse']])
            
            # åˆ†ç±»å™¨å´©æºƒæŒ‡æ ‡
            recent_collapses = [m for m in self.metrics['classifier_collapse'] if m['round'] == latest_round]
            if recent_collapses:
                collapse_count = sum([1 for m in recent_collapses if m['is_collapsed']])
                avg_entropy = np.mean([m['normalized_entropy'] for m in recent_collapses])
                avg_active_classes = np.mean([m['num_active_classes'] for m in recent_collapses])
                
                wandb_logger.log({
                    "diagnostic/classifier_collapses": collapse_count,
                    "diagnostic/avg_prediction_entropy": avg_entropy,
                    "diagnostic/avg_active_classes": avg_active_classes,
                    "round": latest_round
                })
            
            # ç‰¹å¾è´¨é‡æŒ‡æ ‡
            recent_quality = [m for m in self.metrics['shared_layer_quality'] if m['round'] == latest_round]
            if recent_quality:
                avg_quality = np.mean([m['quality_score'] for m in recent_quality])
                avg_dead_ratio = np.mean([m['dead_neuron_ratio'] for m in recent_quality])
                
                wandb_logger.log({
                    "diagnostic/avg_feature_quality": avg_quality,
                    "diagnostic/avg_dead_neuron_ratio": avg_dead_ratio,
                    "round": latest_round
                })
            
            # æƒé‡åˆ†å¸ƒæŒ‡æ ‡
            recent_weights = [m for m in self.metrics['weight_distribution'] if m['round'] == latest_round]
            if recent_weights:
                collapsed_layers = sum([sum([1 for layer_info in m['layers'].values() if layer_info['is_collapsed']]) 
                                      for m in recent_weights])
                wandb_logger.log({
                    "diagnostic/collapsed_weight_layers": collapsed_layers,
                    "round": latest_round
                })
                
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºwandbæŒ‡æ ‡å¤±è´¥: {str(e)}")
    
    # è¾…åŠ©æ–¹æ³•
    def _classify_conflict_level(self, cosine_similarity):
        """åˆ†ç±»å†²çªçº§åˆ«"""
        if cosine_similarity > 0.5:
            return "ååŒ"
        elif cosine_similarity > 0:
            return "è½»å¾®å†²çª"
        elif cosine_similarity > -0.5:
            return "ä¸­ç­‰å†²çª"
        else:
            return "ä¸¥é‡å†²çª"
    
    def _compute_feature_diversity_robust(self, features):
        """ç¨³å¥çš„ç‰¹å¾å¤šæ ·æ€§è®¡ç®—"""
        try:
            if features.size(0) < 2:
                return 0.5
            
            # è®¡ç®—ç‰¹å¾é—´çš„ç›¸å…³æ€§
            normalized_features = F.normalize(features, dim=1)
            correlation_matrix = torch.mm(normalized_features, normalized_features.t())
            
            # æ’é™¤å¯¹è§’çº¿å…ƒç´ 
            mask = torch.eye(correlation_matrix.size(0), device=correlation_matrix.device).bool()
            off_diagonal = correlation_matrix[~mask]
            
            # è®¡ç®—å¤šæ ·æ€§ï¼ˆ1 - å¹³å‡ç›¸å…³æ€§ï¼‰
            avg_correlation = off_diagonal.mean().item()
            diversity = 1.0 - abs(avg_correlation)
            
            return max(0.0, min(1.0, diversity))
            
        except Exception as e:
            return 0.5
    
    def _compute_stability_score(self, avg_change, avg_norm):
        """è®¡ç®—ç¨³å®šæ€§å¾—åˆ†"""
        try:
            if avg_norm < 1e-8:
                return 0.0
            relative_change = avg_change / avg_norm
            stability_score = 1.0 / (1.0 + 10 * relative_change)
            return max(0.0, min(1.0, stability_score))
        except:
            return 0.5

@torch.no_grad()
def compute_client_feature_similarity_robust(client_models, server_feature_extractor, sample_loader, device):
    """ä¿®å¤ç‰ˆå®¢æˆ·ç«¯ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—ï¼Œç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯ä½¿ç”¨è‡ªå·±çš„æœ¬åœ°ç‰¹å¾æå–è·¯å¾„"""
    # å–ä¸€ä¸ªå°æ‰¹æ¬¡åšå¯¹æ¯”
    try:
        images, _ = next(iter(sample_loader))
        images = images.to(device)
    except:
        return 0.5  # è¿”å›ä¸­æ€§å€¼å¦‚æœæ— æ³•è·å–æ•°æ®

    feats = []
    for cid in sorted(client_models.keys()):
        try:
            cm = client_models[cid].to(device)
            cm.eval()
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®¢æˆ·ç«¯æœ¬åœ°çš„å…±äº«/ä¸ªæ€§åŒ–ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç»Ÿä¸€çš„æœåŠ¡å™¨ç‰¹å¾
            local_logits, shared_features, personal_features = cm(images)
            
            # ä½¿ç”¨å®¢æˆ·ç«¯å…±äº«å±‚ç‰¹å¾ä½œä¸ºå®¢æˆ·ç«¯ç‰¹è‰²è¡¨å¾
            f_client = shared_features
            
            # å¯é€‰ï¼šå¦‚æœæœ‰æœåŠ¡å™¨ç‰¹å¾æå–å™¨ï¼Œå†æ¥ä¸Š
            if server_feature_extractor is not None:
                f = server_feature_extractor(f_client)
            else:
                f = f_client
                
            # å±•å¹³ + L2 å½’ä¸€åŒ–ï¼Œé¿å…å°ºåº¦å½±å“
            f = f.view(f.size(0), -1)
            f = torch.nn.functional.normalize(f, dim=1)
            
            # èšæˆä¸€ä¸ª batch çš„å‡å€¼ç‰¹å¾ï¼Œç”¨äºå®¢æˆ·ç«¯çº§åˆ«è¡¨ç¤º
            feats.append(f.mean(dim=0))  # [D]
            
        except Exception as e:
            # å¦‚æœæŸä¸ªå®¢æˆ·ç«¯å‡ºé”™ï¼Œè·³è¿‡
            continue
    
    if len(feats) < 2:
        return 0.5  # éœ€è¦è‡³å°‘ä¸¤ä¸ªå®¢æˆ·ç«¯æ‰èƒ½è®¡ç®—ç›¸ä¼¼åº¦
    
    # è®¡ç®—å®¢æˆ·ç«¯é—´å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
    F = torch.stack(feats, dim=0)  # [K, D]
    sim = torch.mm(F, F.t()).clamp(-1, 1)  # ä½™å¼¦ï¼šå‘é‡å·²å½’ä¸€åŒ–
    
    # æå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆé™¤å»å¯¹è§’çº¿ï¼‰
    upper_indices = torch.triu(torch.ones_like(sim), diagonal=1).bool()
    upper_sim = sim[upper_indices]
    
    if len(upper_sim) == 0:
        return 0.5
        
    avg_similarity = float(upper_sim.mean().item())
    return avg_similarity