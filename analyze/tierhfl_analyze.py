import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import logging
import matplotlib.pyplot as plt  # ä»…åœ¨éœ€è¦å¯è§†åŒ–æ—¶ä½¿ç”¨
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import copy
import os
from datetime import datetime

def analyze_server_features(server_model, client_model, global_test_loader, device='cpu', num_classes=None):
    """åˆ†ææœåŠ¡å™¨æå–ç‰¹å¾çš„å¯åˆ†æ€§"""
    server_model.eval()
    client_model.eval()  # ä½¿ç”¨ä¼ å…¥çš„å•ä¸ªå®¢æˆ·ç«¯æ¨¡å‹
    features_all = []
    labels_all = []
    
    # æ”¶é›†ç‰¹å¾å’Œæ ‡ç­¾
    with torch.no_grad():
        for data, target in global_test_loader:
            data = data.to(device)
            # ä½¿ç”¨ä¼ å…¥çš„å®¢æˆ·ç«¯æ¨¡å‹è·å–å…±äº«å±‚è¾“å‡º
            _, shared_features, _ = client_model(data)
            # é€šè¿‡æœåŠ¡å™¨æ¨¡å‹æå–ç‰¹å¾
            server_features = server_model(shared_features)
            
            features_all.append(server_features.cpu())
            labels_all.append(target)
    
    features_all = torch.cat(features_all, dim=0)
    labels_all = torch.cat(labels_all, dim=0)
    
    # ğŸ”¥ åŠ¨æ€è·å–ç±»åˆ«æ•°
    if num_classes is None:
        num_classes = int(labels_all.max().item()) + 1
    
    # è®¡ç®—ç±»å†…/ç±»é—´è·ç¦»æ¯”
    class_means = {}
    for c in range(num_classes):  # ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç±»åˆ«æ•°
        class_idx = (labels_all == c).nonzero(as_tuple=True)[0]
        if len(class_idx) > 0:  # ç¡®ä¿è¯¥ç±»æœ‰æ ·æœ¬
            class_means[c] = features_all[class_idx].mean(dim=0)
    
    # ç±»å†…è·ç¦»
    intra_class_dist = 0
    num_classes_with_samples = 0
    for c in range(num_classes):  # ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç±»åˆ«æ•°
        class_idx = (labels_all == c).nonzero(as_tuple=True)[0]
        if len(class_idx) > 0:
            class_features = features_all[class_idx]
            intra_class_dist += torch.norm(class_features - class_means[c], dim=1).mean()
            num_classes_with_samples += 1
    
    if num_classes_with_samples > 0:
        intra_class_dist /= num_classes_with_samples
    
    # ç±»é—´è·ç¦»
    inter_class_dist = 0
    count = 0
    classes_with_means = list(class_means.keys())
    for i in range(len(classes_with_means)):
        for j in range(i+1, len(classes_with_means)):
            c1 = classes_with_means[i]
            c2 = classes_with_means[j]
            inter_class_dist += torch.norm(class_means[c1] - class_means[c2])
            count += 1
    
    if count > 0:
        inter_class_dist /= count
    
    separability = inter_class_dist / (intra_class_dist + 1e-8)
    print(f"ç‰¹å¾å¯åˆ†æ€§(ç±»é—´/ç±»å†…è·ç¦»æ¯”): {separability:.4f}")
    
    return separability, features_all, labels_all

def test_with_simple_classifier(server_model, client_model, global_test_loader, device='cpu'):
    """ç”¨ç®€å•åˆ†ç±»å™¨æ›¿ä»£å…¨å±€åˆ†ç±»å™¨æµ‹è¯•ç‰¹å¾è´¨é‡"""
    # æ”¶é›†ç‰¹å¾å’Œæ ‡ç­¾ç”¨äºè®­ç»ƒæ–°åˆ†ç±»å™¨
    features_all = []
    labels_all = []
    
    with torch.no_grad():
        for data, target in global_test_loader:
            data = data.to(device)
            _, shared_features, _ = client_model(data)  # ä½¿ç”¨ä¼ å…¥çš„å®¢æˆ·ç«¯æ¨¡å‹
            server_features = server_model(shared_features)
            
            features_all.append(server_features.cpu())
            labels_all.append(target)
    
    features_train = torch.cat(features_all[:len(features_all)//2], dim=0)
    labels_train = torch.cat(labels_all[:len(labels_all)//2], dim=0)
    features_test = torch.cat(features_all[len(features_all)//2:], dim=0)
    labels_test = torch.cat(labels_all[len(labels_all)//2:], dim=0)
    
    # è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression(max_iter=1000)
    classifier.fit(features_train.numpy(), labels_train.numpy())
    
    # è¯„ä¼°æ–°åˆ†ç±»å™¨
    accuracy = classifier.score(features_test.numpy(), labels_test.numpy()) * 100
    print(f"ç®€å•åˆ†ç±»å™¨å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    return accuracy

def analyze_feature_consistency(server_model, client_models, test_data_dict, device='cpu', num_classes=None):
    """åˆ†æä¸åŒå®¢æˆ·ç«¯é—´ç‰¹å¾çš„ä¸€è‡´æ€§"""
    server_model = server_model.to(device)
    server_model.eval()
    
    # å¯¹æ‰€æœ‰å®¢æˆ·ç«¯çš„ç‰¹å¾è¿›è¡Œåˆ†æ
    client_features = {}
    client_labels = {}
    
    for client_id, test_loader in test_data_dict.items():
        features = []
        labels = []
        
        # ç¡®ä¿å½“å‰å®¢æˆ·ç«¯æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        client_model = client_models[client_id].to(device)
        client_model.eval()
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                _, shared_features, _ = client_model(data)
                server_features = server_model(shared_features)
                
                features.append(server_features.cpu())
                labels.append(target.cpu())
        
        if features:
            client_features[client_id] = torch.cat(features, dim=0)
            client_labels[client_id] = torch.cat(labels, dim=0)
    
    # ğŸ”¥ åŠ¨æ€è·å–ç±»åˆ«æ•°
    if num_classes is None and client_labels:
        all_labels = torch.cat(list(client_labels.values()), dim=0)
        num_classes = int(all_labels.max().item()) + 1
    elif num_classes is None:
        num_classes = 100  # fallback for CIFAR-100
    
    # è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
    stats = {}
    for client_id in client_features:
        feats = client_features[client_id]
        stats[client_id] = {
            'mean': feats.mean().item(),
            'std': feats.std().item(),
            'norm': torch.norm(feats, dim=1).mean().item()
        }
    
    # è®¡ç®—å®¢æˆ·ç«¯é—´ç‰¹å¾ç›¸ä¼¼æ€§
    similarities = {}
    for i in client_features:
        for j in client_features:
            if i != j:
                # è®¡ç®—ç›¸åŒç±»åˆ«æ ·æœ¬çš„ç‰¹å¾ç›¸ä¼¼åº¦
                sim_by_class = {}
                for c in range(num_classes):  # ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç±»åˆ«æ•°
                    i_idx = (client_labels[i] == c).nonzero(as_tuple=True)[0]
                    j_idx = (client_labels[j] == c).nonzero(as_tuple=True)[0]
                    
                    if len(i_idx) > 0 and len(j_idx) > 0:
                        i_feats = client_features[i][i_idx]
                        j_feats = client_features[j][j_idx]
                        
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        i_norm = F.normalize(i_feats, dim=1)
                        j_norm = F.normalize(j_feats, dim=1)
                        
                        # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
                        sim_matrix = torch.mm(i_norm, j_norm.t())
                        sim_score = sim_matrix.max(dim=1)[0].mean().item()
                        sim_by_class[c] = sim_score
                
                if sim_by_class:
                    similarities[f"{i}-{j}"] = sum(sim_by_class.values()) / len(sim_by_class)
    
    # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
    avg_similarity = sum(similarities.values()) / len(similarities) if similarities else 0
    print(f"å®¢æˆ·ç«¯é—´ç‰¹å¾å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    return stats, similarities, avg_similarity

def test_server_compression_ability(server_model, client_models, global_test_loader, device='cpu'):
    """æµ‹è¯•æœåŠ¡å™¨æ¨¡å‹å‹ç¼©éIIDç‰¹å¾çš„èƒ½åŠ›"""
    server_model = server_model.to(device)
    server_model.eval()
    
    # è·å–æœåŠ¡å™¨æ¨¡å‹çš„ä¸­é—´å±‚è¾“å‡º
    activation = {}
    
    def get_activation(name):
        def hook(model, input, output):
            activation[name] = output.detach()
        return hook
    
    # æ³¨å†Œé’©å­å‡½æ•°è·å–ä¸­é—´å±‚è¾“å‡º
    hooks = []
    layers = [module for name, module in server_model.named_modules() 
             if isinstance(module, (nn.Conv2d, nn.Linear))]
    
    for i, layer in enumerate(layers):
        hooks.append(layer.register_forward_hook(get_activation(f'layer_{i}')))
    
    # å¯¹éšæœºé€‰å–çš„æµ‹è¯•æ ·æœ¬è¿›è¡Œå‰å‘ä¼ æ’­
    with torch.no_grad():
        data, _ = next(iter(global_test_loader))
        data = data.to(device)
        
        # è·å–ä¸åŒå®¢æˆ·ç«¯çš„å…±äº«å±‚ç‰¹å¾
        client_shared_features = {}
        for client_id, model in client_models.items():
            model = model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            model.eval()
            _, shared_feats, _ = model(data)
            client_shared_features[client_id] = shared_feats
        
        # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„ç‰¹å¾è®¡ç®—æœåŠ¡å™¨å„å±‚è¾“å‡º
        client_activations = {}
        for client_id, shared_feats in client_shared_features.items():
            # æ¸…ç©ºä¹‹å‰çš„æ¿€æ´»
            activation.clear()
            
            # å‰å‘ä¼ æ’­
            server_model(shared_feats)
            
            # ä¿å­˜è¯¥å®¢æˆ·ç«¯çš„æ¿€æ´»å€¼
            client_activations[client_id] = {k: v.clone().cpu() for k, v in activation.items()}
    
    # æ¸…é™¤é’©å­
    for hook in hooks:
        hook.remove()
    
    # è®¡ç®—æ¯å±‚ç‰¹å¾çš„å®¢æˆ·ç«¯é—´ç›¸ä¼¼åº¦
    layer_similarities = {}
    for layer_name in next(iter(client_activations.values())).keys():
        # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯è¯¥å±‚çš„æ¿€æ´»å€¼
        layer_acts = {}
        for client_id, acts in client_activations.items():
            layer_acts[client_id] = acts[layer_name].view(acts[layer_name].size(0), -1)
        
        # è®¡ç®—å®¢æˆ·ç«¯é—´è¯¥å±‚è¾“å‡ºçš„ç›¸ä¼¼åº¦
        similarities = []
        clients = list(layer_acts.keys())
        for i in range(len(clients)):
            for j in range(i+1, len(clients)):
                ci, cj = clients[i], clients[j]
                
                # æ‰å¹³åŒ–å¹¶æ ‡å‡†åŒ–
                acts_i = F.normalize(layer_acts[ci], dim=1)
                acts_j = F.normalize(layer_acts[cj], dim=1)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                sim = torch.mm(acts_i, acts_j.t()).diag().mean().item()
                similarities.append(sim)
        
        # è¯¥å±‚çš„å¹³å‡ç›¸ä¼¼åº¦
        if similarities:
            layer_similarities[layer_name] = sum(similarities) / len(similarities)
    
    # è®¡ç®—æœåŠ¡å™¨å±‚é—´ç›¸ä¼¼åº¦å˜åŒ–ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨æœåŠ¡å™¨å†…å¢åŠ äº†ä¸€è‡´æ€§
    for i in range(len(layer_similarities) - 1):
        layer1 = f'layer_{i}'
        layer2 = f'layer_{i+1}'
        if layer1 in layer_similarities and layer2 in layer_similarities:
            diff = layer_similarities[layer2] - layer_similarities[layer1]
            print(f"{layer1} -> {layer2} å®¢æˆ·ç«¯é—´ç›¸ä¼¼åº¦å˜åŒ–: {diff:.4f}")
    
    return layer_similarities

def test_client_identity_encoding(server_model, client_models, test_data_dict, device='cpu'):
    """æµ‹è¯•æœåŠ¡å™¨ç‰¹å¾ä¸­æ˜¯å¦åŒ…å«å®¢æˆ·ç«¯èº«ä»½ä¿¡æ¯"""
    server_model = server_model.to(device)
    server_model.eval()
    
    # æ”¶é›†å„å®¢æˆ·ç«¯ç‰¹å¾
    client_features = []
    client_ids = []
    
    for client_id, test_loader in test_data_dict.items():
        features = []
        
        # ç¡®ä¿å½“å‰å®¢æˆ·ç«¯æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        client_model = client_models[client_id].to(device)
        client_model.eval()
        
        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(device)
                _, shared_features, _ = client_model(data)
                server_features = server_model(shared_features)
                
                features.append(server_features.cpu())
        
        if features:
            client_features.append(torch.cat(features, dim=0))
            client_ids.extend([client_id] * len(features))
    
    features_all = torch.cat(client_features, dim=0)
    client_ids = np.array(client_ids)
    
    # è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨æ¥é¢„æµ‹å®¢æˆ·ç«¯èº«ä»½
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    
    X_train, X_test, y_train, y_test = train_test_split(
        features_all.numpy(), client_ids, test_size=0.3, random_state=42)
    
    classifier = SVC()
    classifier.fit(X_train, y_train)
    
    # è¯„ä¼°é¢„æµ‹å®¢æˆ·ç«¯èº«ä»½çš„å‡†ç¡®ç‡
    accuracy = classifier.score(X_test, y_test) * 100
    print(f"ä»ç‰¹å¾é¢„æµ‹å®¢æˆ·ç«¯èº«ä»½çš„å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # éšæœºåˆ†ç±»çš„å‡†ç¡®ç‡ä½œä¸ºåŸºå‡†
    random_accuracy = 100 / len(set(client_ids))
    print(f"éšæœºçŒœæµ‹å®¢æˆ·ç«¯èº«ä»½çš„å‡†ç¡®ç‡: {random_accuracy:.2f}%")
    
    return accuracy, random_accuracy

def validate_server_effectiveness(args, client_models, server_model, global_classifier,
                                 global_test_loader, test_data_local_dict, device='cpu'):
    """é›†æˆéªŒè¯æœåŠ¡å™¨ç‰¹å¾æå–æœ‰æ•ˆæ€§çš„å‡½æ•°"""
    print("\n===== éªŒè¯æœåŠ¡å™¨ç‰¹å¾æå–æœ‰æ•ˆæ€§ =====")
    
    # ğŸ”¥ åŠ¨æ€è·å–ç±»åˆ«æ•°
    if hasattr(args, 'dataset') and args.dataset == 'cifar100':
        num_classes = 100
    elif hasattr(args, 'dataset') and args.dataset == 'cifar10':
        num_classes = 10
    else:
        # ä» global_classifier è·å–ç±»åˆ«æ•°
        try:
            for module in global_classifier.modules():
                if isinstance(module, torch.nn.Linear):
                    num_classes = module.out_features
                    break
            else:
                num_classes = 100  # fallback
        except:
            num_classes = 100  # fallback
    
    print(f"ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç±»åˆ«æ•°: {num_classes}")
    
    # ç¡®ä¿æœåŠ¡å™¨æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    server_model = server_model.to(device)
    
    # é€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯æ¨¡å‹ç”¨äºæµ‹è¯•
    sample_client_id = list(client_models.keys())[0]
    sample_client_model = client_models[sample_client_id].to(device)
    
    try:
        # 1. ç‰¹å¾å¯åˆ†æ€§åˆ†æ
        separability, features, labels = analyze_server_features(
            server_model, sample_client_model, global_test_loader, device=device, num_classes=num_classes)
    except Exception as e:
        print(f"ç‰¹å¾å¯åˆ†æ€§åˆ†æå‡ºé”™: {str(e)}")
        separability = 0.0
    
    try:
        # 2. æ›¿æ¢åˆ†ç±»å™¨æµ‹è¯•
        new_classifier_acc = test_with_simple_classifier(
            server_model, sample_client_model, global_test_loader, device=device)
    except Exception as e:
        print(f"æ›¿æ¢åˆ†ç±»å™¨æµ‹è¯•å‡ºé”™: {str(e)}")
        new_classifier_acc = 0.0
    
    try:
        # 3. ç‰¹å¾ä¸€è‡´æ€§è·¨å®¢æˆ·ç«¯åˆ†æ
        feature_stats, similarities, avg_similarity = analyze_feature_consistency(
            server_model, client_models, test_data_local_dict, device=device, num_classes=num_classes)
    except Exception as e:
        print(f"ç‰¹å¾ä¸€è‡´æ€§åˆ†æå‡ºé”™: {str(e)}")
        avg_similarity = 0.0
        similarities = {}
        feature_stats = {}
    
    try:
        # 4. æœåŠ¡å™¨æ¨¡å‹å‹ç¼©èƒ½åŠ›æµ‹è¯•
        layer_similarities = test_server_compression_ability(
            server_model, client_models, global_test_loader, device=device)
    except Exception as e:
        print(f"æœåŠ¡å™¨å‹ç¼©èƒ½åŠ›æµ‹è¯•å‡ºé”™: {str(e)}")
        layer_similarities = {}
    
    # æš‚æ—¶è·³è¿‡å¯èƒ½æœ‰é—®é¢˜çš„å®¢æˆ·ç«¯èº«ä»½ç¼–ç æµ‹è¯•
    identity_acc = 0.0
    random_acc = 0.0
    identity_leakage = 0.0
    
    print(f"\nå·²è·å–æœ‰æ•ˆçš„éªŒè¯æŒ‡æ ‡:")
    print(f"1. ç‰¹å¾å¯åˆ†æ€§: {separability:.4f}")
    print(f"2. ç®€å•åˆ†ç±»å™¨å‡†ç¡®ç‡: {new_classifier_acc:.2f}%")
    print(f"3. å®¢æˆ·ç«¯é—´ç‰¹å¾å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    # åŸºäºå·²æœ‰æ•°æ®åšå‡ºè¯„ä¼°
    feature_quality_score = (separability * 0.4 + (new_classifier_acc / 100) * 0.6) 
    
    print("\n===== æœåŠ¡å™¨ç‰¹å¾æå–èƒ½åŠ›è¯„ä¼° =====")
    print(f"ç‰¹å¾è´¨é‡å¾—åˆ†(0-1): {feature_quality_score:.4f}")
    print(f"æ•°æ®å¼‚è´¨æ€§é€‚åº”èƒ½åŠ›(0-1): {avg_similarity:.4f}")
    
    if feature_quality_score > 0.3:
        print("ç»“è®º: æœåŠ¡å™¨ç‰¹å¾æå–å·¥ä½œæ­£å¸¸ï¼Œä½†å¯èƒ½éœ€è¦ä¼˜åŒ–ä»¥æ›´å¥½é€‚åº”æ•°æ®å¼‚è´¨æ€§")
    elif new_classifier_acc > 20:
        print("ç»“è®º: æœåŠ¡å™¨æå–çš„ç‰¹å¾æœ‰ä¸€å®šåŒºåˆ†èƒ½åŠ›ï¼Œä½†å…¨å±€åˆ†ç±»å™¨å¯èƒ½å­˜åœ¨é—®é¢˜")
    else:
        print("ç»“è®º: æœåŠ¡å™¨ç‰¹å¾æå–å­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼Œæ— æ³•æä¾›æœ‰æ•ˆç‰¹å¾")
        
    return {
        'feature_quality': feature_quality_score,
        'heterogeneity_adaptation': avg_similarity,
        'simple_classifier_acc': new_classifier_acc
    }



class GlobalClassifierVerifier:
    """å…¨å±€åˆ†ç±»å™¨é—®é¢˜è¯Šæ–­å·¥å…·"""
    
    def __init__(self, server_model, global_classifier, client_models, 
                 global_test_loader, test_data_dict, device='cpu'):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.server_model = server_model.to(device)
        self.global_classifier = global_classifier.to(device)
        self.client_models = {k: v.to(device) for k, v in client_models.items()}
        self.global_test_loader = global_test_loader
        self.test_data_dict = test_data_dict
        self.device = device
        
        # åˆ›å»ºç»“æœç›®å½•
        self.result_dir = f"classifier_verification_{datetime.now().strftime('%m%d_%H%M')}"
        os.makedirs(self.result_dir, exist_ok=True)
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        print("\n===== å…¨å±€åˆ†ç±»å™¨è¯Šæ–­ =====")
        
        # 1. åŸºå‡†æ€§èƒ½æµ‹è¯•
        self.test_baseline_performance()
        
        # 2. æ›¿æ¢åˆ†ç±»å™¨æµ‹è¯•
        self.test_alternative_classifiers()
        
        # 3. ç‰¹å¾åˆ†å¸ƒåˆ†æ
        self.analyze_feature_distribution()
        
        # 4. åˆ†ç±»å™¨æƒé‡åˆ†æ
        self.analyze_classifier_weights()
        
        # 5. æ¢¯åº¦æµåˆ†æ
        self.analyze_gradient_flow()
        
        # 6. æµ‹è¯•æ³›åŒ–èƒ½åŠ›
        self.test_generalization()
        
        print("\n===== è¯Šæ–­å®Œæˆ =====")
    
    def test_baseline_performance(self):
        """æµ‹è¯•åŸºå‡†æ€§èƒ½"""
        print("\n1. åŸºå‡†æ€§èƒ½æµ‹è¯•")
        
        # åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šæµ‹è¯•åŸå§‹åˆ†ç±»å™¨
        original_acc = self._evaluate_on_global_test(
            self.server_model, self.global_classifier)
            
        # åœ¨å„å®¢æˆ·ç«¯æµ‹è¯•é›†ä¸Šæµ‹è¯•
        client_accs = {}
        for client_id, test_loader in self.test_data_dict.items():
            client_model = self.client_models[client_id]
            acc = self._evaluate_on_client_test(
                client_model, self.server_model, self.global_classifier, test_loader)
            client_accs[client_id] = acc
            
        avg_client_acc = sum(client_accs.values()) / len(client_accs)
        
        print(f"åŸå§‹å…¨å±€åˆ†ç±»å™¨åœ¨IIDæµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡: {original_acc:.2f}%")
        print(f"åŸå§‹å…¨å±€åˆ†ç±»å™¨åœ¨å®¢æˆ·ç«¯æµ‹è¯•é›†ä¸Šå¹³å‡å‡†ç¡®ç‡: {avg_client_acc:.2f}%")
        
        return original_acc, client_accs
    
    def test_alternative_classifiers(self):
        """æµ‹è¯•æ›¿ä»£åˆ†ç±»å™¨æ¶æ„"""
        print("\n2. æ›¿ä»£åˆ†ç±»å™¨æµ‹è¯•")
        
        # è·å–ç‰¹å¾ç»´åº¦
        feature_dim = None
        for param in self.global_classifier.parameters():
            if len(param.shape) > 1:
                feature_dim = param.shape[1]
                break
        
        if not feature_dim:
            for name, module in self.global_classifier.named_modules():
                if isinstance(module, nn.Linear):
                    feature_dim = module.in_features
                    break
        
        if not feature_dim:
            print("æ— æ³•ç¡®å®šç‰¹å¾ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼128")
            feature_dim = 128
            
        # ç¡®å®šç±»åˆ«æ•°é‡
        num_classes = 10  # å‡è®¾æ˜¯CIFAR-10
        
        # å®šä¹‰å‡ ç§ä¸åŒçš„åˆ†ç±»å™¨æ¶æ„
        classifiers = {
            "çº¿æ€§åˆ†ç±»å™¨": nn.Linear(feature_dim, num_classes).to(self.device),
            "å•éšå±‚MLP": nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)
            ).to(self.device),
            "åŒéšå±‚MLP": nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, num_classes)
            ).to(self.device),
            "BatchNormç‰ˆMLP": nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)
            ).to(self.device)
        }
        
        # ä»å…¨å±€æµ‹è¯•é›†æ”¶é›†ç‰¹å¾
        all_features, all_labels = self._collect_features_from_global_test()
        
        # ç”¨æ”¶é›†çš„ç‰¹å¾è®­ç»ƒå’Œè¯„ä¼°æ¯ç§åˆ†ç±»å™¨
        results = {}
        for name, classifier in classifiers.items():
            # è®­ç»ƒåˆ†ç±»å™¨
            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)
            classifier.train()
            
            # ä½¿ç”¨80%æ•°æ®è®­ç»ƒï¼Œ20%æµ‹è¯•
            split_idx = int(0.8 * len(all_features))
            train_features, train_labels = all_features[:split_idx], all_labels[:split_idx]
            test_features, test_labels = all_features[split_idx:], all_labels[split_idx:]
            
            # ç®€å•è®­ç»ƒ
            for epoch in range(50):
                optimizer.zero_grad()
                logits = classifier(train_features)
                loss = F.cross_entropy(logits, train_labels)
                loss.backward()
                optimizer.step()
                
                if (epoch + 1) % 10 == 0:
                    print(f"  {name} è®­ç»ƒè½®æ¬¡ {epoch+1}/50, æŸå¤±: {loss.item():.4f}")
            
            # è¯„ä¼°
            classifier.eval()
            with torch.no_grad():
                logits = classifier(test_features)
                _, preds = logits.max(1)
                accuracy = (preds == test_labels).float().mean().item() * 100
                
                # åˆ†ç±»åˆ†å¸ƒ
                pred_dist = torch.zeros(num_classes)
                for i in range(num_classes):
                    pred_dist[i] = (preds == i).sum().item()
                pred_dist = pred_dist / pred_dist.sum()
            
            results[name] = {
                "accuracy": accuracy,
                "pred_distribution": pred_dist.cpu().numpy()
            }
            print(f"  {name} å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"  é¢„æµ‹åˆ†å¸ƒ: {dict(enumerate(pred_dist.cpu().numpy().round(2)))}")
        
        return results
    
    def analyze_feature_distribution(self):
        """åˆ†æç‰¹å¾åˆ†å¸ƒ"""
        print("\n3. ç‰¹å¾åˆ†å¸ƒåˆ†æ")
        
        # æ”¶é›†å…¨å±€æµ‹è¯•é›†å’Œå®¢æˆ·ç«¯æµ‹è¯•é›†çš„ç‰¹å¾
        global_features, global_labels = self._collect_features_from_global_test()
        
        client_features_dict = {}
        client_labels_dict = {}
        for client_id, test_loader in self.test_data_dict.items():
            features, labels = self._collect_features_from_client(client_id, test_loader)
            client_features_dict[client_id] = features
            client_labels_dict[client_id] = labels
        
        # ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
        global_mean = global_features.mean(dim=0)
        global_std = global_features.std(dim=0)
        
        client_means = {}
        client_stds = {}
        for client_id, features in client_features_dict.items():
            client_means[client_id] = features.mean(dim=0)
            client_stds[client_id] = features.std(dim=0)
        
        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯ä¸å…¨å±€ç‰¹å¾çš„åˆ†å¸ƒå·®å¼‚
        print("ç‰¹å¾åˆ†å¸ƒKLæ•£åº¦:")
        for client_id in client_means:
            # è®¡ç®—å‡å€¼å·®å¼‚çš„L2è·ç¦»
            mean_dist = torch.norm(client_means[client_id] - global_mean).item()
            # è®¡ç®—æ ‡å‡†å·®æ¯”ç‡
            std_ratio = (client_stds[client_id] / (global_std + 1e-8)).mean().item()
            
            print(f"  å®¢æˆ·ç«¯ {client_id} - å‡å€¼è·ç¦»: {mean_dist:.4f}, æ ‡å‡†å·®æ¯”ç‡: {std_ratio:.4f}")
        
        # ä½¿ç”¨PCAé™ç»´å¯è§†åŒ–
        pca = PCA(n_components=2)
        
        # å¯¹å…¨å±€ç‰¹å¾è¿›è¡Œé™ç»´
        global_features_np = global_features.cpu().numpy()
        global_pca = pca.fit_transform(global_features_np)
        
        # å®¢æˆ·ç«¯ç‰¹å¾æ˜ å°„åˆ°ç›¸åŒç©ºé—´
        client_pca = {}
        for client_id, features in client_features_dict.items():
            client_pca[client_id] = pca.transform(features.cpu().numpy())
        
        # è·å–åˆ†ç±»å™¨æœ€åä¸€å±‚æƒé‡è¿›è¡ŒæŠ•å½±
        weights = None
        for layer in self.global_classifier.modules():
            if isinstance(layer, nn.Linear) and layer.out_features == 10:  # å‡è®¾10åˆ†ç±»
                weights = layer.weight.detach().cpu().numpy()
                break
        
        # å¦‚æœæ‰¾åˆ°æƒé‡ï¼Œå°†å…¶æŠ•å½±åˆ°PCAç©ºé—´
        if weights is not None:
            weights_pca = pca.transform(weights)
            
            # åˆ›å»ºå†³ç­–è¾¹ç•Œå¯è§†åŒ–
            plt.figure(figsize=(12, 10))
            
            # ç»˜åˆ¶å…¨å±€ç‰¹å¾
            plt.scatter(global_pca[:, 0], global_pca[:, 1], c=global_labels.cpu().numpy(), 
                       cmap='tab10', alpha=0.5, marker='o', s=20, label='å…¨å±€æµ‹è¯•æ ·æœ¬')
            
            # ç»˜åˆ¶ç±»åˆ«è¾¹ç•Œæ–¹å‘
            for i, (x, y) in enumerate(weights_pca):
                plt.arrow(0, 0, x*3, y*3, head_width=0.3, head_length=0.3, fc=f'C{i}', ec=f'C{i}')
                plt.text(x*3.1, y*3.1, f'ç±»åˆ«{i}', fontsize=12)
            
            plt.title('ç‰¹å¾åˆ†å¸ƒä¸åˆ†ç±»å™¨å†³ç­–è¾¹ç•Œ')
            plt.xlabel('PCAç»„ä»¶1')
            plt.ylabel('PCAç»„ä»¶2')
            plt.legend()
            plt.savefig(f"{self.result_dir}/feature_distribution.png", dpi=300)
        
        return {
            "global_stats": (global_mean.cpu().numpy(), global_std.cpu().numpy()),
            "client_stats": {k: (v.cpu().numpy(), client_stds[k].cpu().numpy()) 
                            for k, v in client_means.items()}
        }
        
    def analyze_classifier_weights(self):
        """åˆ†æåˆ†ç±»å™¨æƒé‡"""
        print("\n4. åˆ†ç±»å™¨æƒé‡åˆ†æ")
        
        # æ”¶é›†åŸå§‹åˆ†ç±»å™¨æƒé‡
        original_weights = {}
        with torch.no_grad():
            for name, param in self.global_classifier.named_parameters():
                original_weights[name] = param.data.clone()
                
                # è®¡ç®—å¹¶æ‰“å°æƒé‡ç»Ÿè®¡ä¿¡æ¯
                if param.dim() > 1:  # åªåˆ†ææƒé‡çŸ©é˜µï¼Œä¸åˆ†æåç½®
                    w_mean = param.mean().item()
                    w_std = param.std().item()
                    w_min = param.min().item()
                    w_max = param.max().item()
                    w_norm = torch.norm(param).item()
                    
                    # è®¡ç®—è¡Œå’Œåˆ—ä¹‹é—´çš„æ–¹å·®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®šåå¥½
                    row_means = param.mean(dim=1)
                    col_means = param.mean(dim=0)
                    row_std = row_means.std().item()
                    col_std = col_means.std().item()
                    
                    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                    print(f"  å±‚ {name}:")
                    print(f"    å½¢çŠ¶: {param.shape}")
                    print(f"    å‡å€¼: {w_mean:.4f}, æ ‡å‡†å·®: {w_std:.4f}")
                    print(f"    æœ€å°å€¼: {w_min:.4f}, æœ€å¤§å€¼: {w_max:.4f}")
                    print(f"    èŒƒæ•°: {w_norm:.4f}")
                    print(f"    è¾“å‡ºæ–¹å·®: {row_std:.4f}, è¾“å…¥æ–¹å·®: {col_std:.4f}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å´©æºƒæ¨¡å¼çš„ç—•è¿¹
                    rows_similar = row_std < 0.01
                    cols_similar = col_std < 0. 
                    if rows_similar:
                        print("    è­¦å‘Š: ä½è¾“å‡ºæ–¹å·®å¯èƒ½å¯¼è‡´å´©æºƒåˆ†ç±»")
                    if cols_similar:
                        print("    è­¦å‘Š: ä½è¾“å…¥æ–¹å·®è¡¨æ˜ç‰¹å¾åˆ©ç”¨ä¸è¶³")
                    
                    # åˆ†ææ¯ä¸ªç±»åˆ«çš„æƒé‡
                    if param.shape[0] == 10:  # å‡è®¾æ˜¯æœ€ç»ˆåˆ†ç±»å±‚
                        for i in range(param.shape[0]):
                            class_weight = param[i]
                            class_mean = class_weight.mean().item()
                            class_std = class_weight.std().item()
                            class_norm = torch.norm(class_weight).item()
                            print(f"    ç±»åˆ« {i}: å‡å€¼={class_mean:.4f}, æ ‡å‡†å·®={class_std:.4f}, "
                                 f"èŒƒæ•°={class_norm:.4f}")
        
        # å¦‚æœå¯èƒ½ï¼Œåˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„åˆ†ç±»å™¨ä½œä¸ºå‚è€ƒ
        try:
            reference_classifier = copy.deepcopy(self.global_classifier)
            # é‡æ–°åˆå§‹åŒ–
            for module in reference_classifier.modules():
                if isinstance(module, nn.Linear):
                    nn.init.xavier_uniform_(module.weight)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                        
            # æ”¶é›†å‚è€ƒåˆ†ç±»å™¨æƒé‡
            reference_weights = {}
            with torch.no_grad():
                for name, param in reference_classifier.named_parameters():
                    reference_weights[name] = param.data.clone()
            
            # æ¯”è¾ƒè®­ç»ƒåä¸åˆå§‹åŒ–çš„å·®å¼‚
            print("\n  è®­ç»ƒååˆ†ç±»å™¨ä¸éšæœºåˆå§‹åŒ–æ¯”è¾ƒ:")
            for name in original_weights:
                if original_weights[name].dim() > 1:
                    orig_norm = torch.norm(original_weights[name]).item()
                    ref_norm = torch.norm(reference_weights[name]).item()
                    diff_norm = torch.norm(original_weights[name] - reference_weights[name]).item()
                    
                    print(f"    å±‚ {name}: å½“å‰èŒƒæ•°={orig_norm:.4f}, éšæœºèŒƒæ•°={ref_norm:.4f}, "
                         f"å·®å¼‚èŒƒæ•°={diff_norm:.4f}, ç›¸å¯¹å˜åŒ–={(diff_norm/ref_norm):.4f}")
        except Exception as e:
            print(f"  æ— æ³•åˆ›å»ºå‚è€ƒåˆ†ç±»å™¨: {str(e)}")
        
        return original_weights
    
    def analyze_gradient_flow(self):
        """åˆ†ææ¢¯åº¦æµåŠ¨æƒ…å†µ"""
        print("\n5. æ¢¯åº¦æµåˆ†æ")
        
        # æ”¶é›†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        sample_data, sample_labels = next(iter(self.global_test_loader))
        sample_data, sample_labels = sample_data.to(self.device), sample_labels.to(self.device)
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼å¹¶æ¸…é™¤æ¢¯åº¦
        self.server_model.train()
        self.global_classifier.train()
        
        # é€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯æ¨¡å‹
        client_id = list(self.client_models.keys())[0]
        client_model = self.client_models[client_id].train()
        
        # å‰å‘ä¼ æ’­
        _, shared_features, _ = client_model(sample_data)
        server_features = self.server_model(shared_features)
        logits = self.global_classifier(server_features)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits, sample_labels)
        
        # æ¸…é™¤å…ˆå‰æ¢¯åº¦
        for model in [client_model, self.server_model, self.global_classifier]:
            for param in model.parameters():
                if param.grad is not None:
                    param.grad.zero_()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ”¶é›†æ¢¯åº¦
        gradient_stats = {
            "classifier": {},
            "server": {},
            "client": {}
        }
        
        print("  åˆ†ç±»å™¨æ¢¯åº¦ç»Ÿè®¡:")
        for name, param in self.global_classifier.named_parameters():
            if param.grad is not None:
                grad_norm = torch.norm(param.grad).item()
                param_norm = torch.norm(param.data).item()
                grad_ratio = grad_norm / (param_norm + 1e-8)
                
                gradient_stats["classifier"][name] = {
                    "grad_norm": grad_norm,
                    "param_norm": param_norm,
                    "grad_ratio": grad_ratio
                }
                
                print(f"    {name}: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}, å‚æ•°èŒƒæ•°={param_norm:.4f}, "
                     f"æ¯”ä¾‹={grad_ratio:.6f}")
        
        print("\n  æœåŠ¡å™¨æ¨¡å‹æ¢¯åº¦ç»Ÿè®¡:")
        for name, param in self.server_model.named_parameters():
            if param.grad is not None:
                grad_norm = torch.norm(param.grad).item()
                param_norm = torch.norm(param.data).item()
                grad_ratio = grad_norm / (param_norm + 1e-8)
                
                gradient_stats["server"][name] = {
                    "grad_norm": grad_norm,
                    "param_norm": param_norm,
                    "grad_ratio": grad_ratio
                }
                
                if grad_norm > 0.0001:  # åªæ‰“å°æœ‰æ˜æ˜¾æ¢¯åº¦çš„å‚æ•°
                    print(f"    {name}: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}, å‚æ•°èŒƒæ•°={param_norm:.4f}, "
                         f"æ¯”ä¾‹={grad_ratio:.6f}")
        
        # è®¡ç®—æ¢¯åº¦ç»Ÿè®¡
        clf_grad_norms = [v["grad_norm"] for v in gradient_stats["classifier"].values()]
        server_grad_norms = [v["grad_norm"] for v in gradient_stats["server"].values()]
        
        print("\n  æ¢¯åº¦æµæ€»ç»“:")
        print(f"    åˆ†ç±»å™¨å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(clf_grad_norms):.6f}")
        print(f"    æœåŠ¡å™¨å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(server_grad_norms):.6f}")
        
        # æ£€æŸ¥æ¢¯åº¦æ¯”ä¾‹
        clf_grad_ratios = [v["grad_ratio"] for v in gradient_stats["classifier"].values()]
        print(f"    åˆ†ç±»å™¨æœ€å¤§æ¢¯åº¦/å‚æ•°æ¯”ä¾‹: {max(clf_grad_ratios):.6f}")
        print(f"    åˆ†ç±»å™¨æœ€å°æ¢¯åº¦/å‚æ•°æ¯”ä¾‹: {min(clf_grad_ratios):.6f}")
        
        return gradient_stats
    
    def test_generalization(self):
        """æµ‹è¯•æ³›åŒ–èƒ½åŠ›"""
        print("\n6. æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
        
        # è®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†ç±»å™¨ï¼Œç”¨æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®çš„æ··åˆ
        new_classifier = copy.deepcopy(self.global_classifier)
        optimizer = torch.optim.Adam(new_classifier.parameters(), lr=0.001)
        
        # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯çš„ç‰¹å¾å’Œæ ‡ç­¾
        all_features = []
        all_labels = []
        
        for client_id, test_loader in self.test_data_dict.items():
            features, labels = self._collect_features_from_client(client_id, test_loader)
            all_features.append(features)
            all_labels.append(labels)
        
        # åˆå¹¶æ•°æ®
        if all_features:
            all_features = torch.cat(all_features, dim=0)
            all_labels = torch.cat(all_labels, dim=0)
            
            # è®­ç»ƒæ–°åˆ†ç±»å™¨
            new_classifier.train()
            for epoch in range(30):
                optimizer.zero_grad()
                logits = new_classifier(all_features)
                loss = F.cross_entropy(logits, all_labels)
                loss.backward()
                optimizer.step()
                
                if (epoch + 1) % 10 == 0:
                    print(f"  æ··åˆæ•°æ®è®­ç»ƒè½®æ¬¡ {epoch+1}/30, æŸå¤±: {loss.item():.4f}")
            
            # åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šè¯„ä¼°
            new_classifier.eval()
            
            # æ”¶é›†å…¨å±€æµ‹è¯•é›†ç‰¹å¾
            global_features, global_labels = self._collect_features_from_global_test()
            
            with torch.no_grad():
                logits = new_classifier(global_features)
                _, preds = logits.max(1)
                accuracy = (preds == global_labels).float().mean().item() * 100
                
                # è®¡ç®—åˆ†ç±»åˆ†å¸ƒ
                pred_dist = torch.zeros(10)
                for i in range(10):
                    pred_dist[i] = (preds == i).sum().item()
                pred_dist = pred_dist / pred_dist.sum()
            
            print(f"  åœ¨æ··åˆæ•°æ®ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"  é¢„æµ‹åˆ†å¸ƒ: {dict(enumerate(pred_dist.cpu().numpy().round(2)))}")
            
            # è®¡ç®—åŸå§‹åˆ†ç±»å™¨å‡†ç¡®ç‡ä½œä¸ºå¯¹æ¯”
            original_acc = self._evaluate_on_features(
                self.global_classifier, global_features, global_labels)
            print(f"  åŸå§‹åˆ†ç±»å™¨åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡: {original_acc:.2f}%")
            
            return {
                "mixed_data_accuracy": accuracy,
                "original_accuracy": original_acc,
                "prediction_distribution": pred_dist.cpu().numpy()
            }
        else:
            print("  æ— å®¢æˆ·ç«¯æµ‹è¯•æ•°æ®å¯ç”¨")
            return {}
    
    def _evaluate_on_global_test(self, server_model, classifier):
        """åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        server_model.eval()
        classifier.eval()
        
        # é€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯æ¨¡å‹ç”¨äºç‰¹å¾æå–
        client_id = list(self.client_models.keys())[0]
        client_model = self.client_models[client_id].eval()
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.global_test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                _, shared_features, _ = client_model(data)
                server_features = server_model(shared_features)
                logits = classifier(server_features)
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, pred = logits.max(1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        accuracy = 100.0 * correct / max(1, total)
        return accuracy
    
    def _evaluate_on_client_test(self, client_model, server_model, classifier, test_loader):
        """åœ¨å®¢æˆ·ç«¯æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        client_model.eval()
        server_model.eval()
        classifier.eval()
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                _, shared_features, _ = client_model(data)
                server_features = server_model(shared_features)
                logits = classifier(server_features)
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, pred = logits.max(1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        accuracy = 100.0 * correct / max(1, total)
        return accuracy
    
    def _collect_features_from_global_test(self):
        """ä»å…¨å±€æµ‹è¯•é›†æ”¶é›†ç‰¹å¾"""
        self.server_model.eval()
        
        # é€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯æ¨¡å‹ç”¨äºç‰¹å¾æå–
        client_id = list(self.client_models.keys())[0]
        client_model = self.client_models[client_id].eval()
        
        all_features = []
        all_labels = []
        
        with torch.no_grad():
            for data, target in self.global_test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                # æå–ç‰¹å¾
                _, shared_features, _ = client_model(data)
                server_features = self.server_model(shared_features)
                
                all_features.append(server_features)
                all_labels.append(target)
        
        if all_features:
            all_features = torch.cat(all_features, dim=0)
            all_labels = torch.cat(all_labels, dim=0)
            return all_features, all_labels
        else:
            return torch.tensor([]).to(self.device), torch.tensor([]).to(self.device)
    
    def _collect_features_from_client(self, client_id, test_loader):
        """ä»å®¢æˆ·ç«¯æµ‹è¯•é›†æ”¶é›†ç‰¹å¾"""
        self.server_model.eval()
        client_model = self.client_models[client_id].eval()
        
        all_features = []
        all_labels = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                # æå–ç‰¹å¾
                _, shared_features, _ = client_model(data)
                server_features = self.server_model(shared_features)
                
                all_features.append(server_features)
                all_labels.append(target)
        
        if all_features:
            all_features = torch.cat(all_features, dim=0)
            all_labels = torch.cat(all_labels, dim=0)
            return all_features, all_labels
        else:
            return torch.tensor([]).to(self.device), torch.tensor([]).to(self.device)
    
    def _evaluate_on_features(self, classifier, features, labels):
        """åœ¨é¢„è®¡ç®—çš„ç‰¹å¾ä¸Šè¯„ä¼°åˆ†ç±»å™¨"""
        classifier.eval()
        
        with torch.no_grad():
            logits = classifier(features)
            _, preds = logits.max(1)
            accuracy = (preds == labels).float().mean().item() * 100
        
        return accuracy